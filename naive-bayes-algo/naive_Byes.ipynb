{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive Byes",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqNgTfRvqG87SCA80oiSyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ziva1811/Basics-of-ml/blob/master/naive-bayes-algo/naive_Byes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X88ewYVW0pIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpCMcTF509jQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d3a14c9f-3a72-44fa-e055-80eb2e1efb59"
      },
      "source": [
        "print(\"checking for nltk\")\n",
        "try:\n",
        "    import nltk\n",
        "except ImportError:\n",
        "    print(\"you should install nltk before continuing\")\n",
        "\n",
        "print(\"checking for numpy\")\n",
        "try:\n",
        "    import numpy\n",
        "except ImportError:\n",
        "    print(\"you should install numpy before continuing\")\n",
        "\n",
        "print(\"checking for scipy\")\n",
        "try:\n",
        "    import scipy\n",
        "except:\n",
        "    print(\"you should install scipy before continuing\")\n",
        "\n",
        "print(\"checking for sklearn\")\n",
        "try:\n",
        "    import sklearn\n",
        "except:\n",
        "    print(\"you should install sklearn before continuing\")\n",
        "\n",
        "# print\n",
        "# print \"downloading the Enron dataset (this may take a while)\"\n",
        "# print \"to check on progress, you can cd up one level, then execute <ls -lthr>\"\n",
        "# print \"Enron dataset should be last item on the list, along with its current size\"\n",
        "# print \"download will complete at about 423 MB\"\n",
        "import urllib.request\n",
        "url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\"\n",
        "urllib.request.urlretrieve(url, filename=\"../enron_mail_20150507.tar.gz\") \n",
        "print(\"download complete!\")\n",
        "print(\"unzipping Enron dataset (this may take a while)\")\n",
        "import tarfile\n",
        "import os\n",
        "os.chdir(\"..\")\n",
        "tfile = tarfile.open(\"enron_mail_20150507.tar.gz\", \"r:gz\")\n",
        "tfile.extractall(\".\")\n",
        "\n",
        "print(\"you're ready to go!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checking for nltk\n",
            "checking for numpy\n",
            "checking for scipy\n",
            "checking for sklearn\n",
            "download complete!\n",
            "unzipping Enron dataset (this may take a while)\n",
            "you're ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaJFUmM3Qlk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30747057-c298-4e73-ba97-2b805dec61af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiF4Bl1JQ2aT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6a32a7b0-dfed-4727-902d-147de21ad91b"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/gdrive/My': No such file or directory\n",
            "ls: cannot access 'Drive/Colab': No such file or directory\n",
            "ls: cannot access 'Notebooks/*.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmjFi96Q3LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "b56cb943-b1fa-4f9a-9229-514af0657726"
      },
      "source": [
        "# #!ls '/content/gdrive/My Drive/Colab Notebooks'\n",
        "# !cat '/content/gdrive/My Drive/Colab Notebooks/email_Preprocess.py'"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"email_preprocess\n",
            "\n",
            "Automatically generated by Colaboratory.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1VkIdp4znJNsuEGVEEQ0snRt1e4GJtNTC\n",
            "\"\"\"\n",
            "\n",
            "import pickle\n",
            "#import cPickle\n",
            "import numpy\n",
            "\n",
            "from sklearn.model_selection import cross_val_score\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.feature_selection import SelectPercentile, f_classif\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "def preprocess(words_file = \"/content/gdrive/My Drive/Colab Notebooks/word_data.pkl\", authors_file =\"/content/gdrive/My Drive/Colab Notebooks/email_authors.pkl\"):\n",
            "  authors_file_handler = open(authors_file, \"rb\")\n",
            "  authors = pickle.load(authors_file_handler)\n",
            "  authors_file_handler.close()\n",
            "\n",
            "  words_file_handler = open(words_file, \"rb\")\n",
            "  word_data = pickle.load(words_file_handler)\n",
            "  words_file_handler.close()\n",
            "\n",
            "### test_size is the percentage of events assigned to the test set\n",
            "### (remainder go into training)\n",
            "  features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
            "\n",
            "\n",
            "\n",
            "### text vectorization--go from strings to lists of numbers\n",
            "  vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
            "                             stop_words='english')\n",
            "  features_train_transformed = vectorizer.fit_transform(features_train)\n",
            "  features_test_transformed  = vectorizer.transform(features_test)\n",
            "\n",
            "\n",
            "\n",
            "### feature selection, because text is super high dimensional and \n",
            "### can be really computationally chewy as a result\n",
            "  selector = SelectPercentile(f_classif, percentile=10)\n",
            "  selector.fit(features_train_transformed, labels_train)\n",
            "  features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
            "  features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
            "\n",
            "### info on the data\n",
            "  print (\"no. of Chris training emails:\", sum(labels_train))\n",
            "  print (\"no. of Sara training emails:\", len(labels_train)-sum(labels_train))\n",
            "\n",
            "  return features_train_transformed, features_test_transformed, labels_train, labels_test"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Q0s3px3Mr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7973f4c5-0f99-43d0-9ed0-a61988e19afc"
      },
      "source": [
        "\"\"\" \n",
        "    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. \n",
        "    Use a Naive Bayes Classifier to identify emails by their authors\n",
        "    \n",
        "    authors and labels:\n",
        "    Sara has label 0\n",
        "    Chris has label 1\n",
        "\"\"\"\n",
        "    \n",
        "import sys\n",
        "from time import time\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
        "import email_Preprocess\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = email_Preprocess.preprocess()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "\n",
        "t0 = time()\n",
        "clf.fit(features_train,labels_train)\n",
        "print(\"training time:\", round(time()-t0, 3), \"s\")\n",
        "\n",
        "t1=time()\n",
        "pred= clf.predict(features_test)\n",
        "print(\"prediction time:\", round(time()-t1, 3), \"s\")\n",
        "from sklearn import metrics\n",
        "accuracy = metrics.accuracy_score(labels_test, pred)\n",
        "print(accuracy)\n",
        "\n",
        "\n",
        "#########################################################"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of Chris training emails: 7936\n",
            "no. of Sara training emails: 7884\n",
            "training time: 0.737 s\n",
            "prediction time: 0.077 s\n",
            "0.9732650739476678\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}